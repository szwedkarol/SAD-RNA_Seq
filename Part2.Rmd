---
title: "Part 2"
author: "Karol Szwed"
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        toc_depth: 3
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(eval = FALSE)


library(ggplot2)
library(reshape2)
library(caret)
library(dplyr)
library(glmnet)
library(ranger)
library(goftest)
library(xgboost)

library(foreach)
library(doParallel)
```

# 1. Wczytywanie danych

```{r wczytywanie danych, eval=TRUE}
X_test <- read.csv("X_test.csv")
X_train <- read.csv("X_train.csv")
Y_train <- read.csv("y_train.csv")
```

# 2. Filtrowanie po wariancji

```{r variance filtering, fig.width=12, eval=FALSE}
plot_feature_variance <- function(feature_variance) {
    # Sample of p% of the features for plotting
    p <- 50
    
    sample_size <- max(1, length(feature_variance) * p / 100)
    sampled_indices <- sample(length(feature_variance), sample_size)

    # Data frame for plotting
    plot_data <- data.frame(
        FeatureIndex = sampled_indices,
        Variance = feature_variance[sampled_indices]
    )

    # Plot of the variance of each feature
    plot1 <- (
        ggplot(plot_data, aes(x = FeatureIndex, y = Variance)) +
        geom_point() +
        geom_hline(yintercept = 0.2, color = "red") +
        labs(x = "Feature index", y = "Variance") +
        theme_bw()
    )
    print(plot1)
    
    # Bar plot of the count for each variance bucket
    extended_breaks <- c(seq(0, 1, by = 0.05), Inf)
    variance_buckets <- cut(feature_variance, breaks = extended_breaks, include.lowest = TRUE, right = FALSE)
    levels(variance_buckets)[length(levels(variance_buckets))] <- ">1.0"
    variance_data <- data.frame(VarianceBucket = variance_buckets)
    
    plot2 <- (
        ggplot(variance_data, aes(x = VarianceBucket)) +
        geom_bar() +
        labs(x = "Variance bucket", y = "Count") +
        theme_bw()
    )
    print(plot2)
}


filter_features_by_variance <- function(dataSet, variance_threshold = 0.1, plot = TRUE) {
    # Variance of each feature
    feature_variance <- apply(dataSet, 2, var)

    # Plot of the variance of each feature
    if (plot) plot_feature_variance(feature_variance)

    selected_features <- which(feature_variance > variance_threshold)
    filtered_dataSet <- dataSet[, c(selected_features), drop = FALSE]
    return(filtered_dataSet)
}

#X_dataSet <- filter_features_by_variance(X_train, variance_threshold = 0.1)
```

# 3. Random Forest

```{r - tworzenie foldów do walidacji krzyżowej, eval=TRUE}
# Tworzy 'k' foldów do walidacji krzyżowej
create_folds <- function(data, k = 10) {
  set.seed(123)  # Ustalamy ziarno dla powtarzalności wyników
  n <- nrow(data)
  indices <- sample(1:n)
  folds <- cut(indices, breaks = k, labels = FALSE)

  result <- lapply(1:k, function(x) {
    test_idx <- which(folds == x)
    train_idx <- setdiff(1:n, test_idx)
    list(train = train_idx, test = test_idx)
  })

  # Zwracamy listę wektorów indeksów
  return(result)
}

# Foldy do walidacji krzyżowej
folds <- create_folds(X_train, k = 10)
```



```{r Random Forest}
#X_dataSet <- X_train

best_num_trees <- 3000
best_depth <- 20
best_mtry <- 600

sum_mse_train <- 0
sum_mse_test <- 0
fold_count <- length(folds)
```



```{r Random Forest CV, cache=TRUE}
for (i in seq_along(folds)) {
    fold <- folds[[i]]
        
    train_indices <- fold$train
    test_indices <- fold$test
        
    # Dzielimy dane na foldy
    X_train_fold <- as.matrix(X_train[train_indices, , drop = FALSE])
    Y_train_fold <- Y_train[train_indices, ]
    X_test_fold <- as.matrix(X_train[test_indices, , drop = FALSE])
    Y_test_fold <- Y_train[test_indices, ]
        
    # Dopasowujemy las losowy
    model <- ranger(
        formula         = Y ~ .,      
        data            = data.frame(Y = Y_train_fold, X_train_fold),
        num.trees       = best_num_trees,
        mtry            = best_mtry,
        max.depth       = best_depth,
        seed            = 1000,         # Dla powtarzalności
        verbose         = TRUE,         # Postępy obliczeń
        num.threads     = 7
    )
        
    # Predykcje dla zbioru treningowego i testowego
    train_predictions <- predict(model, data.frame(X_train_fold))$predictions
    test_predictions <- predict(model, data.frame(X_test_fold))$predictions
    
    # Obliczamy MSE dla zbioru treningowego i testowego
    mse_train <- mean((Y_train_fold - train_predictions)^2)
    mse_test <- mean((Y_test_fold - test_predictions)^2)
    
    sum_mse_train <- sum_mse_train + mse_train
    sum_mse_test <- sum_mse_test + mse_test
}
```



```{r full random forest}
rf <- ranger(
        formula         = Y ~ .,      
        data            = data.frame(Y = Y_train[, ], X_train[, ]),
        num.trees       = best_num_trees,
        mtry            = best_mtry,
        max.depth       = best_depth,
        min.node.size   = 5,    # Default for regression
        seed            = 1234,
        verbose         = TRUE,
        num.threads     = 7
)

rf_predictions <- predict(rf, X_test)$predictions

results_rf <- data.frame(
    Id = 0:(length(rf_predictions) - 1),
    Expected = rf_predictions
)

# Write the results to a CSV file
write.csv(results_rf, "rf_predictions.csv", row.names = FALSE)
```



```{r błędy dla pełnego modelu random forest}
# Obliczamy MSE uśrednionego po wszystkich foldach
avg_mse_train <- sum_mse_train / fold_count
avg_mse_test <- sum_mse_test / fold_count

cat("Average Training MSE:", avg_mse_train, "\n")
cat("Average Validation MSE:", avg_mse_test)
```



```{r preparing data for XGBoost}
#X_train <- X_dataSet

# Parameters for XGBoost model
params <- list(
    booster = "gbtree",
    objective = "reg:squarederror",  # Use reg:squarederror for regression tasks
    eta = 0.01,                       # Learning rate
    max_depth = 6,                   # Depth of trees
    subsample = 0.5,                 # Subsampling of the training instances
    colsample_bytree = 0.5           # Subsample ratio of columns when constructing each tree
)
```



```{r training XGBoost}
# Initialize list to store predictions for each fold
predictions_list <- list()
mse_list <- numeric(length(folds))  # To store MSE for each fold

for (i in seq_along(folds)) {
    fold <- folds[[i]]
    
    # Split the data into training and validation sets
    train_indices <- fold$train
    test_indices <- fold$test
    
    # Create DMatrices for train and test sets
    dtrain <- xgb.DMatrix(data = as.matrix(X_train[train_indices, ]),
                          label = Y_train[train_indices, ])
    dtest <- xgb.DMatrix(data = as.matrix(X_train[test_indices, ]),
                         label = Y_train[test_indices, ])
    
    # Number of rounds for XGBoost
    num_rounds <- 3000
    
    # Train model
    bst_model <- xgb.train(params = params,
                           data = dtrain,
                           nrounds = num_rounds,
                           watchlist = list(train = dtrain, test = dtest),
                           print_every_n = 100,
                           eval_metric = "rmse",
                           nthread = 7)

    # Predict on validation set
    predictions <- predict(bst_model, dtest)
    predictions_list[[i]] <- predictions

    # Calculate and store MSE for this fold
    mse_list[i] <- mean((Y_train[test_indices, ] - predictions)^2)
}

# Calculate average MSE across all folds
average_mse <- mean(mse_list)
print(paste("Average MSE across all folds: ", average_mse))


```



```{r full XGBoost}
# Number of rounds for XGBoost
num_rounds <- 3000
    
# Create DMatrices for train and test sets
dtrain <- xgb.DMatrix(data = as.matrix(X_train[, ]),
                          label = Y_train[, ])

dtest <- xgb.DMatrix(data = as.matrix(X_test[, ]))

# Train model
bst_model <- xgb.train(params = params,
                           data = dtrain,
                           nrounds = num_rounds,
                           watchlist = list(train = dtrain),
                           print_every_n = 100,
                           eval_metric = "rmse",
                           nthread = 7)

xgb_pred <- predict(bst_model, dtest)

results_xgb <- data.frame(
    Id = 0:(length(xgb_pred) - 1),
    Expected = xgb_pred
)

# Write the results to a CSV file
write.csv(results_xgb, "xgb_predictions.csv", row.names = FALSE)

```


```{r avg of xgboost and random forest}
avg_pred <- (xgb_pred + rf_predictions) / 2

print(median(rf_predictions))
print(median(xgb_pred))
print(median(avg_pred))

results_avg <- data.frame(
    Id = 0:(length(avg_pred) - 1),
    Expected = avg_pred
)

# Write the results to a CSV file
write.csv(results_avg, "avg_predictions.csv", row.names = FALSE)

```


