---
title: "Part 2"
author: "Karol Szwed"
date: "`r Sys.Date()`"
output:
    html_document:
        toc: true
        toc_depth: 3
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
knitr::opts_chunk$set(eval = FALSE)


library(ggplot2)
library(caret)
library(dplyr)
library(ranger)
library(xgboost)
library(mlr3)
library(mlr3learners)
library(mlr3filters)
library(mlr3fselect)
library(mlr3pipelines)

library(foreach)
library(doParallel)
```

# 1. Wczytywanie danych

```{r wczytywanie danych, eval=TRUE}
X_test <- read.csv("X_test.csv")
X_train_full <- read.csv("X_train.csv")
Y_train <- read.csv("y_train.csv")
```

# 2. Filtrowanie po wariancji

Aby poprawić tempo nauki oraz pozbyć się zmiennych głównie stanowiących wprowadzających szum, stosuję najpierw filtrowanie po wariancji zmiennej.
Tym samym pozbywam się zmiennych o wariancji poniżej 0.05, co stanowi ok. 50% wszystkich zmiennych objaśniających.

```{r variance filtering, fig.width=12, eval=TRUE}
plot_feature_variance <- function(feature_variance) {
    # Sample of p% of the features for plotting
    p <- 50
    
    sample_size <- max(1, length(feature_variance) * p / 100)
    sampled_indices <- sample(length(feature_variance), sample_size)

    # Data frame for plotting
    plot_data <- data.frame(
        FeatureIndex = sampled_indices,
        Variance = feature_variance[sampled_indices]
    )

    # Plot of the variance of each feature
    plot1 <- (
        ggplot(plot_data, aes(x = FeatureIndex, y = Variance)) +
        geom_point() +
        geom_hline(yintercept = 0.2, color = "red") +
        labs(x = "Feature index", y = "Variance") +
        theme_bw()
    )
    print(plot1)
    
    # Bar plot of the count for each variance bucket
    extended_breaks <- c(seq(0, 1, by = 0.05), Inf)
    variance_buckets <- cut(feature_variance, breaks = extended_breaks, include.lowest = TRUE, right = FALSE)
    levels(variance_buckets)[length(levels(variance_buckets))] <- ">1.0"
    variance_data <- data.frame(VarianceBucket = variance_buckets)
    
    plot2 <- (
        ggplot(variance_data, aes(x = VarianceBucket)) +
        geom_bar() +
        labs(x = "Variance bucket", y = "Count") +
        theme_bw()
    )
    print(plot2)
}


filter_features_by_variance <- function(dataSet, variance_threshold = 0.1, plot = TRUE) {
    # Variance of each feature
    feature_variance <- apply(dataSet, 2, var)

    # Plot of the variance of each feature
    if (plot) plot_feature_variance(feature_variance)

    selected_features <- which(feature_variance > variance_threshold)
    filtered_dataSet <- dataSet[, c(selected_features), drop = FALSE]
    return(filtered_dataSet)
}

X_train <- X_train_full
X_train <- filter_features_by_variance(X_train, variance_threshold = 0.05)
```

# 3. Filtrowanie po _information gain_ oraz _impurity_ na podstawie lasów losowych.

Stosuję te filtry w celu znalezienia zmiennych, na podstawie których najlepiej modele będą w stanie się uczyć. Tym samym pozbywam się zmiennych majających
mały wpływ na wartość zmiennej objaśnianej.

```{r mlr3 filters, eval=TRUE}
select_top_features <- function(task, n_features = 500) {
    n_each <- ceiling(n_features / 2)
  
    # Information gain filter
    filter = flt("information_gain")
    filter$calculate(task)
    print(head(filter$scores))

    # Sort scores in decreasing order and select top features
    scores <- filter$scores
    top_features <- names(sort(scores, decreasing = TRUE)[1:n_each])
    
    # Random Forest importance for regression
    learner <- lrn("regr.ranger", importance = "impurity")
    learner$train(task)
    importance_scores <- learner$importance()
    
    print(head(sort(importance_scores, decreasing = TRUE)[1:n_each]))
    
    top_features <- c(top_features, names(sort(importance_scores, decreasing = TRUE)[1:n_each]))

    # Return the names of the top features
    return(top_features)
}

# Create a classification task
names(Y_train) <- "target_CD36"
task <- TaskRegr$new(id = "my_data", backend = data.frame(X_train, Y_train), target = "target_CD36")

top_feat <- select_top_features(task, n_features = 500)

X_train <- X_train[, top_feat]
```



# 4. Lasy losowe

Skorzystam m. in. z modelu lasów losowych, gdyż już podczas 1. części projektu dawał on obiecujące rezultaty. Spodziewamy się uzyskać tym lepszy wynik,
gdy dodatkowo zastosujemy filtrowanie po _information gain_ oraz _impurity_.

```{r - tworzenie foldów do walidacji krzyżowej, eval=FALSE}
# Tworzy 'k' foldów do walidacji krzyżowej
create_folds <- function(data, k = 10) {
  set.seed(123)  # Ustalamy ziarno dla powtarzalności wyników
  n <- nrow(data)
  indices <- sample(1:n)
  folds <- cut(indices, breaks = k, labels = FALSE)

  result <- lapply(1:k, function(x) {
    test_idx <- which(folds == x)
    train_idx <- setdiff(1:n, test_idx)
    list(train = train_idx, test = test_idx)
  })

  # Zwracamy listę wektorów indeksów
  return(result)
}

# Foldy do walidacji krzyżowej
folds <- create_folds(X_train, k = 10)
```

Zapisujemy parametry, które okazały się najlepsze podczas walidacji krzyżowej.

```{r Random Forest}
best_num_trees <- 1000
best_depth <- 30
best_mtry <- 250

sum_mse_train <- 0
sum_mse_test <- 0
fold_count <- length(folds)
```

Aby wyłonić najlepsze parametry skorzystamy z tej samej procedury, co w punkcie $5)$ pierwszej części projektu.

```{r Random Forest CV, cache=TRUE}
for (i in seq_along(folds)) {
    fold <- folds[[i]]
        
    train_indices <- fold$train
    test_indices <- fold$test
        
    # Dzielimy dane na foldy
    X_train_fold <- as.matrix(X_train[train_indices, , drop = FALSE])
    Y_train_fold <- Y_train[train_indices, ]
    X_test_fold <- as.matrix(X_train[test_indices, , drop = FALSE])
    Y_test_fold <- Y_train[test_indices, ]
        
    # Dopasowujemy las losowy
    model <- ranger(
        formula         = Y ~ .,      
        data            = data.frame(Y = Y_train_fold, X_train_fold),
        num.trees       = best_num_trees,
        mtry            = best_mtry,
        max.depth       = best_depth,
        seed            = 1000,         # Dla powtarzalności
        verbose         = TRUE,         # Postępy obliczeń
        num.threads     = 7
    )
        
    # Predykcje dla zbioru treningowego i testowego
    train_predictions <- predict(model, data.frame(X_train_fold))$predictions
    test_predictions <- predict(model, data.frame(X_test_fold))$predictions
    
    # Obliczamy MSE dla zbioru treningowego i testowego
    mse_train <- mean((Y_train_fold - train_predictions)^2)
    mse_test <- mean((Y_test_fold - test_predictions)^2)
    
    sum_mse_train <- sum_mse_train + mse_train
    sum_mse_test <- sum_mse_test + mse_test
}
```

Uczymy teraz model na pełnych danych z użyciem najlepszych wyłonionych parametrów.

```{r full random forest}
rf <- ranger(
        formula         = Y ~ .,      
        data            = data.frame(Y = Y_train[, ], X_train[, ]),
        num.trees       = best_num_trees,
        mtry            = best_mtry,
        max.depth       = best_depth,
        min.node.size   = 5,    # Default for regression
        seed            = 1234,
        verbose         = TRUE,
        num.threads     = 7
)

# Scaling the data
X_test_scaled_flt <- scale(X_test[, top_feat], center = TRUE)
rf_predictions <- predict(rf, X_test_scaled_flt)$predictions

results_rf <- data.frame(
    Id = 0:(length(rf_predictions) - 1),
    Expected = rf_predictions
)

# Write the results to a CSV file
write.csv(results_rf, "rf_predictions.csv", row.names = FALSE)
```



```{r błędy dla pełnego modelu random forest}
# Obliczamy MSE uśrednionego po wszystkich foldach
avg_mse_train <- sum_mse_train / fold_count
avg_mse_test <- sum_mse_test / fold_count

cat("Average Training MSE:", avg_mse_train, "\n")
cat("Average Validation MSE:", avg_mse_test)
```

# 5. Model _eXtreme Gradient Boosting_

Z uwagi na dużą złożoność danych (wiele zmiennych), małą próbką oraz potencjalne nieoczywiste zależności pomiędzy zmiennymi, korzystam z modelu
_XGBoost_ w wersji dla zadania regresji.

Niżej najlepszy zestaw parametrów wyłoniony podczas walidacji krzyżowej. W trakcie testów oraz na podstawie wyników na Kaggle
okazało się, iż dobrze sobie radzą modele w pewnym stopniu przeuczone na zbiorze treningowym.

```{r preparing data for XGBoost}
X_train <- scale(X_train, center = TRUE)

# Parameters for XGBoost model
params <- list(
    booster = "gbtree",
    objective = "reg:squarederror",
    eta = 0.01,                      # Learning rate
    max_depth = 7,                   # Depth of trees
    subsample = 0.8,                 # Subsampling of the training instances
    colsample_bytree = 0.8           # Subsample ratio of columns when constructing each tree
)
```

Metoda do walidacji krzyżowej modelu XGBoost, analogiczna co dla modelu lasów losowych.

```{r CV for XGBoost}
# Initialize list to store predictions for each fold
predictions_list <- list()
mse_list <- numeric(length(folds))  # To store MSE for each fold

for (i in seq_along(folds)) {
    fold <- folds[[i]]
    
    # Split the data into training and validation sets
    train_indices <- fold$train
    test_indices <- fold$test
    
    # Create DMatrices for train and test sets
    dtrain <- xgb.DMatrix(data = as.matrix(X_train[train_indices, ]),
                          label = Y_train[train_indices, ])
    dtest <- xgb.DMatrix(data = as.matrix(X_train[test_indices, ]),
                         label = Y_train[test_indices, ])
    
    # Number of rounds for XGBoost
    num_rounds <- 3000
    
    # Train model
    bst_model <- xgb.train(params = params,
                           data = dtrain,
                           nrounds = num_rounds,
                           watchlist = list(train = dtrain, test = dtest),
                           print_every_n = 100,
                           eval_metric = "rmse",
                           nthread = 7)

    # Predict on validation set
    predictions <- predict(bst_model, dtest)
    predictions_list[[i]] <- predictions

    # Calculate and store MSE for this fold
    mse_list[i] <- mean((Y_train[test_indices, ] - predictions)^2)
}

# Calculate average MSE across all folds
average_mse <- mean(mse_list)
print(paste("Average MSE across all folds: ", average_mse))
```

Model _XGBoost_ czasami zwraca ujemną predykcję dla zmiennej objaśnianej, która przyjmuje tylko wartości nieujmne, stąd wszystkie predykcje
ujemne zamieniamy na zero.

```{r postprocess of predictions for xgboost}
# True values for this prediction task are non-negative
adjust_predictions <- function(predictions) {
  # Replace negative values with 0
  predictions[predictions < 0] <- 0
  return(predictions)
}
```

Teraz trenujemy pełny model z najlepszym znaleziomym zestawem parametrów.

```{r full XGBoost}
# Number of rounds for XGBoost
num_rounds <- 2500
    
# Create DMatrices for train and test sets
dtrain <- xgb.DMatrix(data = as.matrix(X_train[, ]),
                          label = Y_train[, ])

X_test_scaled_flt <- scale(X_test[, top_feat], center = TRUE)

# Prediction is done only based on the top features
dtest_filtered <- xgb.DMatrix(data = as.matrix(X_test_scaled_flt))

# Train model
bst_model <- xgb.train(params = params,
                           data = dtrain,
                           nrounds = num_rounds,
                           watchlist = list(train = dtrain),
                           print_every_n = 100,
                           eval_metric = "rmse",
                           nthread = 7)

xgb_pred <- predict(bst_model, dtest_filtered)
```

Zapisujemy wyniki (po ich skorygowaniu) do pliku CSV.

```{r saving predictions to a file}
# xgb_pred <- read.csv("xgb_predictions.csv")
xgb_pred <- adjust_predictions(xgb_pred)

results_xgb <- data.frame(
    Id = 0:(length(xgb_pred) - 1),
    Expected = xgb_pred
)

# Write the results to a CSV file
write.csv(results_xgb, "xgb_predictions.csv", row.names = FALSE)
```

Niewielką poprawę wyniku dawało również uśrednienie modelu _XGBoost_ oraz lasów losowych.

```{r avg of xgboost and random forest}
avg_pred <- (xgb_pred + rf_predictions) / 2

results_avg <- data.frame(
    Id = 0:(length(avg_pred) - 1),
    Expected = avg_pred
)

# Write the results to a CSV file
write.csv(results_avg, "avg_predictions.csv", row.names = FALSE)
```


