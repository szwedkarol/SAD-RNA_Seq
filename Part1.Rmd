---
title: "Part 1"
author: "Karol Szwed"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)

library(ggplot2)
library(reshape2)
library(caret)
library(dplyr)
library(glmnet)
library(ranger)

library(foreach)
library(doParallel)
```

## Eksploracja danych

```{r wczytywanie danych}
X_test <- read.csv("X_test.csv")
X_train <- read.csv("X_train.csv")
Y_train <- read.csv("y_train.csv")
```

### A) Rozmiar danych i typ zmiennych

Widzimy, że dane treningowe składają się z $6800$ obserwacji i $9000$ zmiennych objaśniających. Dane testowe składają się z $1200$ obserwacji i $9000$ zmiennych objaśniających. Zmienna objaśniana (białko $CD36$) jest jednowymiarowa. Wszystkie zmienne są typu 'double', co jest zgodne z oczekiwaniami. Tym samym nie dokonujemy konwersji typów zmiennych. Nie ma braków danych.

```{r podstawowa eksploracja danych}
print("Dane treningowe - RNA")
print(dim(X_train))

print("Dane treningowe - białko powierzchniowe")
print(dim(Y_train))

print("Dane testowe - RNA")
print(dim(X_test))
```

```{r typ zmiennych i braki danych}
# Typ zmiennych - wszystkie powinny być typu 'double'
print(all(sapply(X_test, is.double)))
print(all(sapply(X_train, is.double)))
print(all(sapply(Y_train, is.double)))

# Sprawdzenie czy są braki danych
print(any(sapply(X_test, is.na)))
print(any(sapply(X_train, is.na)))
print(any(sapply(Y_train, is.na)))
```

### B) Rozkład zmiennej objaśnianej

Zmienna objaśniana (białko $CD36$) jest zmienną ciągłą. Na histogramie widać, że dla wartości mniejszych od $0.5$ występuje duża liczba obserwacji. Wykres kwantylowy pokazuje, że rozkład zmiennej objaśnianej jest zbliżony do rozkładu normalnego, z wyjątkiem wartości skrajnie małych.

```{r podpunkt b - rozkład zmiennej objaśnianej}
# Badamy rozkład zmiennej objaśnianej (dane 'Y_train')
summary(Y_train)

# Histogram
ggplot(data = Y_train, aes(x = CD36)) + geom_histogram(color="black", fill="white")

# Wykres kwantylowy
ggplot(data = Y_train) + geom_qq(aes(sample = CD36), size=1.5, color="red")

```
### C) Korelacja zmiennych

Obrazujemy korelację zmiennych na mapie ciepła. Wybieramy $250$ zmiennych o największej korelacji z zmienną objaśnianą. Widać, że niektóre zmienne są ze sobą mocno skorelowane.

```{r podpunkt c - korelacja zmiennych}
# Korelacja zmiennych
# Wybieramy 250 zmiennych o największej korelacji z zmienną objaśnianą
correlated <- apply(X_train, 2, function (x) cor(x, Y_train))
X_high_cor <- X_train[, order(correlated, decreasing = T)[1:250]]

# Macierz korelacji
cor_matrix <- cor(X_high_cor)
cor_relations <- melt(cor_matrix)

# Ilustrujemy wynik za pomocą mapy ciepła
library(viridis)
Var.heatmap <- ggplot(data = cor_relations, mapping = aes(x = Var1,
                                                          y = Var2,
                                                          fill = value)) +
  geom_tile() + scale_fill_viridis(discrete=FALSE) +
  theme(axis.text.x = element_blank(), axis.text.y = element_blank()) +
  coord_fixed()
print(Var.heatmap)
```

## Testy statystyczne


```{r Punkt 2 - WIP}
ggplot(data = Y_train) +
  geom_qq(aes(sample = CD36), size=1.5, color = "red") +
  geom_qq_line(aes(sample = CD36))


ks.test(Y_train, "pnorm") # Test Kolmogorova-Smiernov'a


# Wybieramy zmienne najbardziej skorelowane
correlations <- apply(X_train, 2, function (x) cor(x, Y_train))
most_correlated <- X_train[,order(correlations, decreasing = T)[1]]

# Test statystyczny hipotezy zgodności z rozkładem
ggplot() + aes(x = most_correlated) + 
  geom_histogram(binwidth = 0.1, color="blue", fill="white")

# Normalizacja danych
most_normalized <- (most_correlated - mean(most_correlated)) / sd(most_correlated)

ks.test(most_normalized, "pnorm") # Is it normal? It should be, except this pike in 0
most_cleared <- most_normalized[most_correlated > 0.2]
ks.test(most_cleared, "pnorm")
```

## ElasticNet

### A) Opis modelu ElasticNet

_ElasticNet_ jest metodą regularyzowanej regresji, która liniowo łączy w sobie dwa rodzaje regularyzacji: $\ell_1$ (_LASSO_) i $\ell_2$ (_Ridge_). Regularyzacja $\ell_1$ redukuje współczynniki modelu do zera, co pozwala na selekcję zmiennych. Regularyzacja $\ell_2$ z kolei zmniejsza wartości współczynników, ale nie redukuje ich do zera. Dzięki temu model _ElasticNet_ łączy w sobie zalety obu metod, co pozwala na selekcję zmiennych oraz zmniejszenie wariancji modelu.

Model ten jest szczególnie przydatny przy dużych zbiorach danych, które zawierają wiele zmiennych współliniowych lub kiedy liczba zmiennych jest większa od liczby obserwacji. W naszym przypadku, gdzie mamy do czynienia z $9000$ zmiennymi i $6800$ obserwacjami, model _ElasticNet_ może okazać się bardzo przydatny.

Model _ElasticNet_ optymalizuje parametry regresji $\beta$. Te współczynniki obrazują zależność między zmiennymi objaśniającymi a zmienną objaśnianą.
Model ten minimalizuje następującą funkcję straty:

$$ \underset{\beta}{\min} \left\{ \frac{1}{2n} \sum_{i=1}^n (y_i - X_i \beta)^2 + \lambda \left( \alpha \sum_{j=1}^p |\beta_j| + \frac{1}{2} (1 - \alpha) \sum_{j=1}^p \beta_j^2 \right) \right\} $$

gdzie:
- $n$ - liczba obserwacji.
- $y_i$ - zmienna objaśniana.
- $X_i$ - wektor zmiennych objaśniających.
- $\beta$ - wektor współczynników.
- $\lambda$ - parametr regularyzacji.
- $\alpha$ - parametr mieszający, gdzie $ 0 \leq \alpha \leq 1 $.

Dla $\alpha = 0$, _ElasticNet_ odpowiada regresji grzbietowej, a dla $\alpha = 1$, model odpowiada regresji _LASSO_.

### B) Tuning hiperparametrów

Definiujemy funkcję, która tworzy zadaną liczbę foldów w zbiorze danych przekazanym jako argument.

```{r - tworzenie foldów do walidacji krzyżowej}
# Tworzy 'k' foldów do walidacji krzyżowej
create_folds <- function(data, k = 10) {
  set.seed(123)  # Ustalamy ziarno dla powtarzalności wyników
  n <- nrow(data)
  indices <- sample(1:n)
  folds <- cut(indices, breaks = k, labels = FALSE)

  result <- lapply(1:k, function(x) {
    test_idx <- which(folds == x)
    train_idx <- setdiff(1:n, test_idx)
    list(train = train_idx, test = test_idx)
  })

  # Zwracamy listę wektorów indeksów
  return(result)
}
```

Chcemy zminimalizować prawdopodobieństwo, że w danym foldzie duża część wartości $y_i$ jest równa $0$ lub jest blisko zera. Stąd dzielimy dane na $5$ foldów.

```{r - tuning hiperparametrów - STARA WERSJA}

# Foldy do walidacji krzyżowej
folds <- create_folds(X_train, k = 10)

# Siatka hiperparametrów
alphas <- c(0, 0.1, 0.5, 0.9, 1)  # 0 dla Ridge, 1 dla Lasso
lambdas <- 10^seq(1, -2, length = 10)  # od 10 do 0.01

# Pusta lista do zapisania wyników CV
results <- list()

# Przechodzimy przez wszystkie kombinacje hiperparametrów
for (alpha in alphas) {
  for (lambda in lambdas) {
    fold_metrics <- numeric(length(folds))
    
    for (i in seq_along(folds)) {
      fold <- folds[[i]]
      
      # Indeksy treningowe i testowe
      train_indices <- fold$train
      test_indices <- fold$test
      
      # Dzielimy dane
      X_train_fold <- as.matrix(X_train[train_indices, , drop = FALSE])
      Y_train_fold <- Y_train[train_indices, ]
      X_test_fold <- as.matrix(X_train[test_indices, , drop = FALSE])
      Y_test_fold <- Y_train[test_indices, ]
      
      # Tworzymy model ElasticNet
      model <- glmnet(X_train_fold, Y_train_fold, alpha = alpha, lambda = lambda, trace.it = TRUE)
      
      # Predykcje dla zbioru testowego
      predictions <- predict(model, s = lambda, newx = X_test_fold)
      
      # MSE dla foldu testowego
      fold_metrics[i] <- mean((Y_test_fold - predictions)^2)
    }
    
    # Zapisujemy wynik
    results[[paste("Alpha", alpha, "Lambda", lambda)]] <- mean(fold_metrics)
  }
}

print(results)

# Wybieramy najlepsze hiperparametry
best_params <- which.min(unlist(results))
results[best_params]

```


```{r - tuning wielowątkowo}
# Ustawiamy liczbę rdzeni
numCores <- parallel::detectCores() - 1
cl <- makeCluster(numCores)
registerDoParallel(cl)

# Foldy do walidacji krzyżowej
folds <- create_folds(X_train, k = 10)

# Siatka hiperparametrów
alphas <- c(0, 0.1, 0.5, 0.9, 1)  # 0 dla Ridge, 1 dla Lasso
lambdas <- 10^seq(1, -2, length = 10)  # od 10 do 0.01

# Run cross-validation in parallel
results <- foreach(alpha = alphas, .combine = 'c', .packages = 'glmnet') %dopar% {
  alpha_results <- list()
  
  for (lambda in lambdas) {
    fold_metrics <- list()
    
    for (i in seq_along(folds)) {
      fold <- folds[[i]]
      
      # Training and testing indices
      train_indices <- fold$train
      test_indices <- fold$test
      
      # Split data
      X_train_fold <- as.matrix(X_train[train_indices, , drop = FALSE])
      Y_train_fold <- Y_train[train_indices, ]
      X_test_fold <- as.matrix(X_train[test_indices, , drop = FALSE])
      Y_test_fold <- Y_train[test_indices, ]
      
      # Fit ElasticNet model
      model <- glmnet(X_train_fold, Y_train_fold, alpha = alpha, lambda = lambda)
      
      # Predictions for the test set
      predictions <- predict(model, s = lambda, newx = X_test_fold)
      
      # MSE for the test fold
      fold_metrics[[i]] <- mean((Y_test_fold - predictions)^2)
    }
    
    # Store MSE values for all folds with current lambda
    alpha_results[[paste("Lambda", lambda)]] <- fold_metrics
  }
  
  # Return results for current alpha
  list(alpha = alpha, Results = alpha_results)
}

# Shut down parallel backend
stopCluster(cl)


print(results)

```

```{r - testy wyników CV}
for (i in 1:10) {
  print(results[i]$alpha)
}

print(results[6]$Results$`Lambda 0.1`[[1]])

print(results[1])

```


```{r wyniki CV}
# Initialize a data frame to store the averaged MSE along with corresponding alpha and lambda
mse_summary <- data.frame(alpha = numeric(), lambda = numeric(), avg_MSE = numeric())

# Loop through the results, handling the structure described
for (i in seq(1, length(results), by = 2)) {
    alpha_value <- results[[i]]
    lambda_results <- results[[i + 1]]  # This should be the 'Results' list for the current alpha

    # Process each lambda result
    for (lambda_name in names(lambda_results)) {
        lambda_value <- as.numeric(sub("Lambda ", "", lambda_name))  # Extract the numeric lambda value
        mse_values <- unlist(lambda_results[[lambda_name]])  # Collect all MSE values for this lambda
        
        # Compute average MSE
        avg_mse <- mean(mse_values)
        
        # Append to the summary dataframe
        mse_summary <- rbind(mse_summary, data.frame(alpha = alpha_value, lambda = lambda_value, avg_MSE = avg_mse))
    }
}

# Print the dataframe to verify correct aggregation
print(head(mse_summary))

# Identify the combination with the lowest average MSE
min_mse_row <- mse_summary[which.min(mse_summary$avg_MSE), ]

# Display the optimal alpha, lambda, and their corresponding MSE
print(min_mse_row)
```

### C) Wykresy skrzypcowe dla MSE


```{r przygotowanie danych przed rysowaniem wykresu}
# Initialize a data frame to store the averaged MSE along with corresponding alpha and lambda
mse_all <- data.frame(alpha = numeric(), lambda = numeric(), MSE = numeric())

# Loop through the results, handling the structure described
for (i in seq(1, length(results), by = 2)) {
    alpha_value <- results[[i]]
    lambda_results <- results[[i + 1]]  # This should be the 'Results' list for the current alpha

    # Process each lambda result
    for (lambda_name in names(lambda_results)) {
        lambda_value <- as.numeric(sub("Lambda ", "", lambda_name))  # Extract the numeric lambda value
        mse_values <- unlist(lambda_results[[lambda_name]])  # Collect all MSE values for this lambda
        
        # Append to the summary dataframe
        for (mse in mse_values) {
            mse_all <- rbind(mse_all, data.frame(alpha = alpha_value, lambda = lambda_value, MSE = mse))
        }
    }
}

# Print the dataframe to verify correct aggregation
print(head(mse_all))

mse_plot <- mse_all %>%
  mutate(alpha_lambda = paste("λ =", format(round(lambda, 3), nsmall = 3)))
```

```{r wykresy skrzypcowe, fig.width=15, fig.height=6}
# Unique alpha values
unique_alphas <- unique(mse_plot$alpha)

# Create and save plots
plots <- list()
for (alpha in unique_alphas) {
  data_subset <- mse_plot[mse_plot$alpha == alpha, ]
  
  p <- ggplot(data_subset, aes(x = alpha_lambda, y = MSE)) +
    geom_violin(trim = FALSE, fill = "lightblue", color = "darkblue") +
    geom_point(position = position_jitter(width = 0.2), size = 1.0, alpha = 0.6) +
    labs(title = paste("Distribution of MSE for Alpha =", alpha),
         x = "",
         y = "Mean Squared Error (MSE)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          plot.title = element_text(size = 14, face = "bold"),
          legend.position = "none")  # Remove legend
  
  # Store the plot in a list
  plots[[paste("Alpha", alpha)]] <- p
}

# To display a specific plot, for example for alpha = 0.1
print(plots[["Alpha 0.5"]])


```

### D) Błąd treningowy i walidacyjny dla optymalnych hiperparametrów


```{r }
print(min_mse_row)
# Fit ElasticNet model


```

## Lasy losowe

```{r hiperparameter grid}
num_trees <- c(100, 200, 300, 400)  # Liczba drzew
max_depth <- c(10, 20, 30, 40)  # Maksymalna głębokość każdego drzewa
mtry_default <- round(sqrt(ncol(X_train)))  # Domyślna wartość 'mtry' to sqrt(liczba zmiennych)

mtry_values <- c(50, mtry_default, 150, 200)
```


```{r ranger random forest - CV, cache=TRUE}
num_rows <- length(num_trees) * length(max_depth) * length(mtry_values) * length(folds)
# Wyniki CV
results_df <- data.frame(
  fold_i = integer(num_rows),
  num_trees = integer(num_rows),
  depth = integer(num_rows),
  mtry = integer(num_rows),
  MSE = numeric(num_rows)
)

counter <- 1
for (trees in num_trees) {
  for (depth in max_depth) {
    for (mtry in mtry_values) {
      for (i in seq_along(folds)) {
        fold <- folds[[i]]
        
        train_indices <- fold$train
        test_indices <- fold$test
        
        # Dzielimy dane na foldy
        X_train_fold <- as.matrix(X_train[train_indices, , drop = FALSE])
        Y_train_fold <- Y_train[train_indices, ]
        X_test_fold <- as.matrix(X_train[test_indices, , drop = FALSE])
        Y_test_fold <- Y_train[test_indices, ]
        
        # Dopasowujemy las losowy
        model <- ranger(
          formula         = Y ~ .,
          data            = data.frame(Y = Y_train_fold, X_train_fold),
          num.trees       = trees,
          mtry            = mtry,
          max.depth       = depth,
          seed            = 1000,         # Dla powtarzalności
          verbose         = TRUE,         # Postępy obliczeń
          num.threads     = 7
        )
        
        # Predykcje dla zbioru testowego
        predictions <- predict(model, data.frame(X_test_fold))$predictions
        
        # Obliczamy MSE dla foldu testowego
        mse_value <- mean((Y_test_fold - predictions)^2)

        # Wyniki zapisujemy we wcześniej zdefiniowanej ramce danych
        results_df[counter, ] <- c(i, trees, depth, mtry, mse_value)
        counter <- counter + 1
        print(counter)
      }
    }
  }
}
```
Oglądamy przykładowe wyniki walidacji krzyżowej oraz zapisujemy je w pliku CSV dla późniejszego wykorzystania.

```{r test wyników CV}
head(results_df)

write.csv(results_df, "random_forest_cv_results.csv", row.names = FALSE)
```

Znajdujemy najlepszy zestaw hiperparametrów na podstawie walidacji krzyżowej.

```{r najlepsze hiperparametry z CV}
average_mse <- results_df %>%
    group_by(num_trees, depth, mtry) %>%
    summarise(avg_MSE = mean(MSE), .groups = 'drop')

head(average_mse)

best_row <- which.min(average_mse$avg_MSE)
print(average_mse[best_row, ])
```

### C) Wykres skrzypcowy

```{r box plot - random forest}
box_plots <- list()
for (number_of_trees in num_trees) {
  # Filtrujemy dane, aby wybrać właściwy podzbiór dla danej liczby drzew
  data_subset <- results_df[results_df$num_trees == number_of_trees, ]
  
  # Tworzymy wykres pudełkowy dla danej liczby drzew
  p <- ggplot(data_subset, aes(x = factor(paste("Depth", depth, "Mtry", mtry)), y = MSE)) +
    geom_boxplot(fill = "lightblue", color = "darkblue") +
    geom_jitter(color = "darkred", size = 1.0, width = 0.2, alpha = 0.6) +
    labs(title = paste("Distribution of MSE for num_trees =", number_of_trees),
         x = "Hyperparameter Combinations (Depth & Mtry)",
         y = "Mean Squared Error (MSE)") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
          plot.title = element_text(size = 14, face = "bold"),
          legend.position = "none")
  
  # Wykresy tworzą listę, gdzie liczba drzew służy jako klucz
  box_plots[[number_of_trees]] <- p
}

print(box_plots[[300]])  # Liczba drzew jest kluczem w liście wykresów
```


